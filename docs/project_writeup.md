# Foundry: AI-Powered Multi-Agent Dataset Creation System

## üìã Project Writeup

---

## 1Ô∏è‚É£ Problem Statement

### The Problem We're Solving

**Dataset creation is the single biggest bottleneck in enterprise Computer Vision workflows.**

Every company deploying CV solutions‚Äîwhether for manufacturing defect detection, retail inventory management, autonomous vehicles, or quality assurance‚Äîfaces the same critical challenge: creating high-quality training datasets.

### Why This Matters

**For businesses building object detection models, dataset creation represents:**

- **100+ hours of manual labor** required to create datasets of meaningful size
- **$500-$1,000 cost per dataset** in annotation team salaries
- **Weeks of delay** in model deployment timelines
- **Limited scalability** as annotation teams become the constraint
- **Small/medium businesses are completely priced out** of custom CV solutions

**Real-World Impact:**

- **Manufacturing:** A factory needs defect detection for a new product line‚Äîmanual annotation delays deployment by 3-4 weeks
- **Quality Assurance:** Companies need rapid iteration on detection models‚Äîmanual annotation creates multi-week feedback loops
- **ML Teams:** Data scientists spend **nearly 30% of project time** on data preparation instead of model development
- **Annotation costs can exceed compute costs** for many projects

### Why This Problem is Important

This bottleneck has far-reaching consequences:

1. **Delayed Innovation:** New CV features take months instead of weeks to deploy
2. **Wasted Talent:** ML engineers spend time on manual labor instead of modeling
3. **Market Inequality:** Only well-funded companies can afford custom CV solutions
4. **Slow Iteration:** Testing new model architectures requires weeks of data prep

**This isn't just an efficiency problem‚Äîit's a fundamental barrier to democratizing Computer Vision.**

---

## 2Ô∏è‚É£ Why Agents?

### Why Agents are the Right Solution

Dataset creation isn't a single task‚Äî**it's a complex, multi-stage workflow where each stage requires different intelligence and tools.**

Traditional approaches use monolithic systems that struggle with this complexity. **Agents provide the perfect solution because:**

### 1. **Specialized Intelligence**
Each stage needs different expertise:
- **Image Discovery:** Search algorithms, API integration, deduplication
- **Quality Filtering:** Vision understanding, relevance assessment
- **Object Detection:** Spatial reasoning, bounding box generation
- **Quality Validation:** Completeness checking, feedback generation
- **Format Transformation:** Data structure conversion, coordinate math

**‚Üí Each agent focuses on one task with optimized prompts and tools**

### 2. **Autonomous Execution**
Agents can:
- Make decisions without human intervention
- Handle errors and retry automatically
- Adapt to different scenarios (single vs. multi-object)
- Manage their own execution loops

**‚Üí True end-to-end automation**

### 3. **Iterative Improvement**
The quality loop demonstrates agent collaboration:
- QualityValidator agent reviews work
- Generates specific, actionable feedback
- AnnotationRefinementLoop manages improvement cycles
- System self-corrects to achieve production quality

**‚Üí Agents enable self-improving workflows**

### 4. **Scalable Parallelism**
Multiple annotator agents work concurrently:
- Process different images simultaneously
- Share workload intelligently
- Coordinate through orchestrator

**‚Üí 3x performance improvement through parallel agents**

### 5. **Meaningful Orchestration**
The Main Agent coordinates complex workflows:
- Parses natural language requests
- Manages execution loops until target count reached
- Routes work to appropriate specialized agents
- Makes high-level decisions (when to mine more, when to stop)

**‚Üí Intelligent workflow management, not just task chaining**

### Why Not Traditional Automation?

- **Scripts:** Can't handle variability, no decision-making
- **Pipelines:** Fixed flow, no adaptation or feedback
- **Monolithic AI:** Jack-of-all-trades, master of none
- **Human-in-the-loop:** Defeats the purpose of automation

**Agents uniquely combine autonomy, specialization, and collaboration.**

---

## 3Ô∏è‚É£ What We Created

Save: data/output/coco.json
```

### Key Technical Features

1. **Natural Language Interface:** Simple prompts like "create 10 images of cats"
2. **Perceptual Hash Deduplication:** Prevents duplicate images (Hamming distance < 5)
3. **Iterative Quality Refinement:** Self-improving annotations
4. **Parallel Processing:** 3x faster with concurrent workers
5. **Flexible Configuration:** YAML + CLI overrides
6. **Comprehensive Error Handling:** Structured errors with retry logic
7. **Rate Limiting:** Token bucket algorithm for API tier compliance
8. **Metrics Collection:** Track performance across all stages

---

## 4Ô∏è‚É£ Demo

### Live Execution Walkthrough

**Command:**
```bash
python pipeline.py --query "dogs" --count 10 --enable-quality-loop
```

**What Happens:**

**[0-10 seconds] Initialization**
```
üöÄ Starting Foundry Pipeline
üìã Configuration loaded
ü§ñ Initializing agents...
   ‚úì Miner Agent ready
   ‚úì Curator Agent ready
   ‚úì Annotator Agent ready (3 workers)
   ‚úì Quality Loop enabled (coordinate validation)
```

**[10-30 seconds] Mining Phase**
```
üîç Miner Agent: Searching for 'dogs'...
   Found 12 image URLs
   Downloading: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 12/12
   Deduplication: 2 duplicates removed
   ‚úì Saved 10 unique images to data/raw/
```

**[30-60 seconds] Curation Phase**
```
‚ú® Curator Agent: Filtering for quality...
   Image 1: ‚úì KEEP (actual dog visible)
   Image 2: ‚úó REJECT (product packaging)
   Image 3: ‚úì KEEP (actual dog visible)
   ...
   ‚úì Kept 8/10 images (80% success rate)
```

**[60-120 seconds] Annotation Phase**
```
üè∑Ô∏è Parallel Annotator: Processing 8 images (3 workers)
   Worker 1: Annotating image_001.jpg... ‚úì
   Worker 2: Annotating image_002.jpg... ‚úì
   Worker 3: Annotating image_003.jpg... ‚úì
   ...
   ‚úì Annotated 8 images
```

**[120-180 seconds] Quality Refinement**
```
üîÑ Quality Loop: Validating annotations...
   Image 1: ‚úì APPROVED (1 dog detected, bbox accurate)
   Image 2: ‚ö†Ô∏è NEEDS_IMPROVEMENT (2nd dog in background missed)
      ‚Üí Feedback: "Second dog in background not detected"
      ‚Üí Re-annotating with feedback...
      ‚Üí ‚úì APPROVED (2 dogs detected)
   ...
   ‚úì 8 images approved (2 iterations average)
```

**[180-200 seconds] Engineering Phase**
```
üì¶ Engineer Module: Transforming to COCO format...
   ‚úì Created categories: [{"id": 1, "name": "dogs"}]
   ‚úì Transformed 8 images, 12 annotations
   ‚úì Saved to data/output/coco.json
```

**[200-210 seconds] Completion**
```
‚úÖ Pipeline Complete!

üìä METRICS SUMMARY:
   Total Images Mined: 10
   Total Images Curated: 8 (80% success)
   Total Images Annotated: 8 (100% success)
   Total Annotations: 12 objects
   
   ‚è±Ô∏è Timings:
   Mining: 20s
   Curation: 30s
   Annotation: 60s
   Quality Loop: 60s
   Engineering: 10s
   Total: 3m 30s

üíæ Output: data/output/coco.json
```

### Visualization

**Command:**
```bash
python visualize_results.py
```

**Output:**
- Opens images with bounding boxes drawn
- Shows object labels
- Displays confidence (if available)
- Saves annotated images to `data/visualized/`

### Example COCO Output

```json
{
  "images": [
    {
      "id": 1,
      "file_name": "image_001.jpg",
      "width": 800,
      "height": 600
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [120, 80, 240, 180],
      "area": 43200
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "dogs",
      "supercategory": "object"
    }
  ]
}
```

---

## 5Ô∏è‚É£ The Build

### How We Created It

**Development Approach:**
1. **Research Phase:** Studied multi-agent patterns, COCO format, Gemini capabilities
2. **Architecture Design:** Designed agent separation of concerns
3. **Iterative Development:** Built agents one at a time, tested individually
4. **Integration:** Connected agents through orchestrator
5. **Quality Enhancement:** Added quality loop for self-improvement
6. **Optimization:** Implemented parallel processing, rate limiting
7. **Testing:** Comprehensive integration tests across scenarios

### Technology Stack

**Core AI/ML:**
- **Google Gemini 2.5 Flash** - Powers all AI agents (vision + language)
- **google-generativeai** - Python SDK for Gemini API
- **google-genai** - Google GenAI SDK
- **google-adk** - Google ADK for agent framework

**APIs & Services:**
- **Google Custom Search API** - Image discovery
- **google-api-python-client** - API client library

**Image Processing:**
- **Pillow (PIL)** - Image manipulation and format conversion
- **ImageHash** - Perceptual hashing for deduplication

**Configuration & Environment:**
- **python-dotenv** - Environment variable management
- **PyYAML** - Configuration file parsing

**Utilities:**
- **requests** - HTTP requests for image downloading
- **Python 3.10+** - Base language with modern features

### Architecture Decisions

**1. Multi-Agent Pattern**
- **Why:** Clear separation of concerns, specialized intelligence
- **Benefit:** Each agent optimized for its task

**2. Gemini 2.5 Flash**
- **Why:** Best-in-class vision + language capabilities
- **Benefit:** Single model for all AI tasks, consistent quality

**3. Iterative Quality Loop**
- **Why:** Automated quality assurance without human review
- **Benefit:** Production-ready annotations automatically

**4. Parallel Processing**
- **Why:** Annotation is the slowest stage
- **Benefit:** 3x performance improvement

**5. YAML Configuration**
- **Why:** User-friendly, hierarchical, overridable
- **Benefit:** Easy customization without code changes

**6. COCO Format**
- **Why:** Industry standard, widely supported
- **Benefit:** Works with all major CV frameworks

### Implementation Highlights

**Perceptual Hash Deduplication:**
```python
import imagehash
from PIL import Image

img_hash = imagehash.phash(image)
for seen_hash in self.seen_hashes:
    if img_hash - seen_hash < 5:  # Hamming distance
        is_duplicate = True
```

**Quality Loop Feedback:**
```python
validation = validator.validate(image_path, bboxes, query)
if validation["status"] == "NEEDS_IMPROVEMENT":
    feedback = validation["feedback"]
    # Re-annotate with feedback
    improved_bboxes = annotator.annotate_with_feedback(
        image_path, query, feedback
    )
```

**Parallel Annotation:**
```python
with ThreadPoolExecutor(max_workers=3) as executor:
    futures = {
        executor.submit(worker.annotate, img, query): img
        for img in images
    }
    for future in as_completed(futures):
        result = future.result()
```

### Challenges Overcome

1. **Rate Limiting:** Implemented token bucket algorithm for API tier compliance
2. **JSON Parsing:** Added auto-fix for common formatting issues (single quotes, trailing commas)
3. **Deduplication:** Perceptual hashing handles slight image variations
4. **Quality Consistency:** Quality loop ensures production-ready results
5. **Cross-Platform:** Removed Windows-specific dependencies for Colab/Kaggle compatibility

---

## 6Ô∏è‚É£ Future Roadmap: From Prototype to Production

While the current architecture successfully demonstrates ADK capabilities, a production-scale version would address the following architectural evolutions:

### 1. True Streaming Architecture (Addressing "False Parallelism")
**Current:** Sequential steps (Mine ‚Üí Curate ‚Üí Annotate) for clear state management.
**Future:** Async event-driven pipeline.
- **Design:** Use a pub/sub model (e.g., Google Pub/Sub).
- **Flow:** Miner pushes to `topic:raw-images` ‚Üí Curator pulls, processes, pushes to `topic:curated-images` ‚Üí Annotator pulls.
- **Benefit:** Zero blocking. Annotation starts the millisecond the first image is curated.

### 2. Decoupled State Management (Addressing "God Object")
**Current:** Shared `PipelineState` object for ease of passing data between ADK tools.
**Future:** Distributed state store (Redis/Firestore).
- **Design:** Agents become stateless workers.
- **Flow:** State is persisted externally; agents accept a `job_id`, fetch context, process, and update state atomically.
- **Benefit:** Horizontal scalability and fault tolerance.

### 3. Optimized Tool Granularity (Addressing "Wrapper Overhead")
**Current:** LLM agents wrap simple tools to demonstrate agentic control.
**Future:** Hybrid "Centaur" approach.
- **Design:** Use LLMs *only* for cognitive tasks (Curation, Annotation).
- **Flow:** Mining becomes a pure Python service (fast, cheap). Curation remains an agent (requires judgment).
- **Benefit:** 90% cost reduction and 10x lower latency for the mining step.

### 4. Active Learning Integration
- **What:** Integrate with model training to identify uncertain predictions.
- **Why:** Focus annotation efforts on most valuable images.
- **Impact:** Improve model performance with fewer annotated images.

### 5. Web-Based UI
- **What:** Build interactive web interface for non-technical users.
- **Why:** Make Foundry accessible to business users, not just developers.
- **Features:** Drag-and-drop BYOD, visual review, progress tracking.

---

## üéØ Summary

**Foundry demonstrates how multi-agent AI systems can automate complex, high-value business workflows.** By reducing dataset creation from 100+ hours to within a day and costs from $1,000 to $10, it removes the biggest bottleneck in enterprise Computer Vision‚Äîmaking CV accessible to companies of all sizes.

**Key Achievements:**
- ‚úÖ **Nearly 50x speed improvement**
- ‚úÖ **98% cost reduction**
- ‚úÖ **Production-ready quality** through iterative refinement
- ‚úÖ **Fully autonomous** end-to-end execution
- ‚úÖ **Open source** for maximum community impact

**The multi-agent architecture, powered by Google Gemini 2.5 Flash, showcases the future of AI-powered workflow automation.**

---

**GitHub Repository:** https://github.com/mauryantitans/Foundry  
**License:** MIT (Open Source)
